{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.append(os.getenv(\"CODE_PATH\"))\n",
    "sys.path.append(os.getenv(\"FIN_DATABASE_PATH\"))\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sqlalchemy.orm import sessionmaker\n",
    "from sqlalchemy import create_engine, select\n",
    "from datetime import datetime, timedelta\n",
    "import logging\n",
    "import matplotlib.pyplot as plt\n",
    "from dotenv import load_dotenv\n",
    "import ta\n",
    "from log_config import setup_logging\n",
    "from Data.connect import engine, DailyStockData, HourlyStockData, OneMinuteStockData, FiveMinuteStockData,FifteenMinuteStockData, StockSplits, StockNews, CompanyFinancials\n",
    "from Pre_Processing.pre_processing import PreProcessing\n",
    "from Feature_Engineering.feature_engineering import TechnicalIndicators\n",
    "from pipeline import Pipeline\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "from sklearn.metrics import root_mean_squared_error as rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tickers = ['AAPL', 'MSFT', 'SPY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../config.yaml'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/raphaelravinet/Code/Volatility_Modelling/intraday_vol.ipynb Cell 3\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/raphaelravinet/Code/Volatility_Modelling/intraday_vol.ipynb#W2sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m pipe \u001b[39m=\u001b[39m Pipeline(tickers)\n",
      "File \u001b[0;32m~/Code/algo_trading/pipeline.py:33\u001b[0m, in \u001b[0;36mPipeline.__init__\u001b[0;34m(self, tickers, config_file)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     31\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtickers \u001b[39m=\u001b[39m tickers\n\u001b[0;32m---> 33\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39;49m(config_file, \u001b[39m'\u001b[39;49m\u001b[39mr\u001b[39;49m\u001b[39m'\u001b[39;49m) \u001b[39mas\u001b[39;00m file:\n\u001b[1;32m     34\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig \u001b[39m=\u001b[39m yaml\u001b[39m.\u001b[39msafe_load(file)\n\u001b[1;32m     36\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mperiods \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig[\u001b[39m'\u001b[39m\u001b[39mtechnical_indicators\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mfast_periods\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig[\u001b[39m'\u001b[39m\u001b[39mtechnical_indicators\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mslow_periods\u001b[39m\u001b[39m'\u001b[39m]\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../config.yaml'"
     ]
    }
   ],
   "source": [
    "pipe = Pipeline(tickers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minute_data = pipe.pipeline(timespan='minute',start_date='2018-01-01',end_date='2019-01-01')\n",
    "hourly_data = pipe.pipeline(timespan='hour',start_date='2018-01-01',end_date='2019-01-01')\n",
    "daily_data = pipe.pipeline(timespan='daily',start_date='2018-01-01',end_date='2019-01-01')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minute_data['AAPL']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Applying the Approach from Young Li's Paper: \n",
    "**\"A Practical Model for Prediction of Intraday Volatility\"**\n",
    "\n",
    "**Potential Improvement:**\n",
    "\n",
    "1. **Change the daily vol model from EWMA to a GARCH-type model**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class IntraBarsProfile:\n",
    "#     def __init__(self, hf_df, lf_df, daily_df, timespan):\n",
    "#         \"\"\"args:\n",
    "#         hf_df: higher frequency dataframe\n",
    "#         lf_df: lower frequency dataframe\n",
    "#         daily_df: daily dataframe\n",
    "#         timespan: timespan of the lower frequency data to aggregate : '15M', '1H', '1D', '1W....'\"\"\"\n",
    "#         self.hf_df = hf_df\n",
    "#         self.lf_df = lf_df\n",
    "#         self.daily_df = daily_df\n",
    "#         self.timespan = timespan\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create a simple dummy dataset with datetime index\n",
    "data = {\n",
    "    'open': [100, 102, 101, 103, 102, 104],\n",
    "    'high': [103, 104, 105, 106, 107, 108],\n",
    "    'low': [99, 100, 99, 101, 101, 102],\n",
    "    'close': [102, 103, 104, 105, 106, 107],\n",
    "}\n",
    "\n",
    "times = pd.date_range('2023-09-02 09:00', periods=6, freq='30T')\n",
    "df = pd.DataFrame(data, index=times)\n",
    "\n",
    "print(\"Original DataFrame:\")\n",
    "print(df)\n",
    "\n",
    "# Floor the times to nearest 30 mins\n",
    "df_grouped = df.groupby(df.index.floor('30T')).aggregate({\n",
    "    'open': 'first',\n",
    "    'high': 'max',\n",
    "    'low': 'min',\n",
    "    'close': 'last'\n",
    "})\n",
    "\n",
    "print(\"\\nGrouped DataFrame (flooring):\")\n",
    "print(df_grouped)\n",
    "\n",
    "# Now let's test using ceil instead of floor\n",
    "df_grouped_ceil = df.groupby(df.index.ceil('30T')).aggregate({\n",
    "    'open': 'first',\n",
    "    'high': 'max',\n",
    "    'low': 'min',\n",
    "    'close': 'last'\n",
    "})\n",
    "\n",
    "print(\"\\nGrouped DataFrame (ceiling):\")\n",
    "print(df_grouped_ceil)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IntradayVol:\n",
    "    def __init__(self, hf_df, lf_df, daily_df, timespan, decay_factor = 0.94):\n",
    "        \"\"\"args:\n",
    "        hf_df: higher frequency dataframe\n",
    "        lf_df: lower frequency dataframe\n",
    "        daily_df: daily dataframe\n",
    "        timespan: timespan of the lower frequency data to aggregate : '15M', '1H', '1D', '1W....'\"\"\"\n",
    "        self.hf_df = hf_df\n",
    "        self.lf_df = lf_df\n",
    "        self.daily_df = daily_df\n",
    "        self.timespan = timespan\n",
    "        self.decay_factor = decay_factor\n",
    "\n",
    "    \n",
    "    def ewma_vol(self):\n",
    "        \"\"\" Calculate daily vol using EWMA model \"\"\"\n",
    "        self.daily_df['daily_vol_squared'] = np.zeros(len(self.daily_df), dtype=float)\n",
    "        \n",
    "        self.daily_df['squared_log_ret'] = self.daily_df['log_ret'] ** 2\n",
    "        self.daily_df['squared_log_ret'].bfill(inplace=True)\n",
    "\n",
    "        for i in range(1, len(self.daily_df)):\n",
    "            self.daily_df.loc[self.daily_df.index[i], 'daily_vol_squared'] = (\n",
    "                self.decay_factor * self.daily_df['daily_vol_squared'].iloc[i-1] +\n",
    "                (1 - self.decay_factor) * self.daily_df['squared_log_ret'].iloc[i]\n",
    "            )\n",
    "        \n",
    "        return np.sqrt(self.daily_df['daily_vol_squared'])\n",
    "    \n",
    "    \n",
    "    def garch_vol(self):\n",
    "        \"\"\" Calculate daily vol using GARCH(1,1) model \"\"\"\n",
    "        self.daily_df.dropna(subset=['return'], inplace=True)\n",
    "        \n",
    "        model = arch_model(self.daily_df['return'] * 100, vol='EGARCH', p=1, q=1)\n",
    "        garch_results = model.fit(disp=\"off\")\n",
    "        \n",
    "        # Extract the conditional volatility from the model\n",
    "        self.daily_df['daily_vol'] = garch_results.conditional_volatility / 100  # scale back down\n",
    "        \n",
    "        return self.daily_df['daily_vol']\n",
    "\n",
    "\n",
    "    \n",
    "    def calculate_diurnal_profile(self):\n",
    "        \"\"\"Calculate diurnal profile. Basically the average of the Garman-Klass vol for each time of the day. \n",
    "        It is calculated using the aggregated data of the higher frequency data\"\"\"\n",
    "        #removing the first datapoint of the day, as it is not a full interval\n",
    "        self.hf_df = self.hf_df[self.hf_df.index.time != pd.Timestamp(\"09:30:00\").time()]\n",
    "\n",
    "        aggregated_df = self.hf_df.groupby(self.hf_df.index.ceil(self.timespan)).aggregate({'open': 'first', \n",
    "                                                               'high': 'max', \n",
    "                                                               'low': 'min', \n",
    "                                                               'close': 'last'})\n",
    "        \n",
    "        \n",
    "        garma_klass_vol = np.sqrt(0.5*(np.log(aggregated_df['high']) - np.log(aggregated_df['low']))**2 - (2 * np.log(2) -1) * (np.log(aggregated_df['close']) - np.log(aggregated_df['open']))**2)\n",
    "        q5 = garma_klass_vol.groupby(aggregated_df.index.time).quantile(0.05)\n",
    "        q25 = garma_klass_vol.groupby(aggregated_df.index.time).quantile(0.25)\n",
    "        q50 = garma_klass_vol.groupby(aggregated_df.index.time).quantile(0.5)\n",
    "        q75 = garma_klass_vol.groupby(aggregated_df.index.time).quantile(0.75)\n",
    "        q95 = garma_klass_vol.groupby(aggregated_df.index.time).quantile(0.95)\n",
    "        \n",
    "        diurnal_profile = (q25 + q50 + q75) / 3\n",
    "        diurnal_profile = diurnal_profile / diurnal_profile.mean()\n",
    "        \n",
    "        return diurnal_profile, q5, q25, q50, q75, q95\n",
    "    \n",
    "    \n",
    "    \n",
    "    def predict_vol(self):\n",
    "        \"\"\" Predict vol for the next time interval\"\"\"\n",
    "        daily_vol = self.ewma_vol().shift(1)\n",
    "        diurnal_profile = self.calculate_diurnal_profile()\n",
    "        avg_diurnal_profile = diurnal_profile.mean()\n",
    "        \n",
    "        # time-scaling factor (rho_t)\n",
    "        time_scaling_factor = 1  \n",
    "\n",
    "        self.lf_df['vol_forecasts'] = np.nan\n",
    "        for i in range(len(self.lf_df)):\n",
    "            current_time = self.lf_df.index[i].time()\n",
    "            current_date = self.lf_df.index[i].date()\n",
    "            \n",
    "            daily_vol_value = daily_vol.loc[pd.Timestamp(current_date)]\n",
    "            if current_time in diurnal_profile.index:\n",
    "                self.lf_df.loc[self.lf_df.index[i], 'vol_forecasts'] = (\n",
    "                    daily_vol_value * time_scaling_factor * (diurnal_profile[current_time] / avg_diurnal_profile)\n",
    "                )\n",
    "            else:\n",
    "                self.lf_df.loc[self.lf_df.index[i], 'vol_forecasts'] = np.nan\n",
    "        return self.lf_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intraday_vol_aapl = IntradayVol(hf_df=minute_data['AAPL'], lf_df=hourly_data['AAPL'], daily_df=daily_data['AAPL'], timespan='30T')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intraday_vol_msft = IntradayVol(hf_df=minute_data['MSFT'], lf_df=hourly_data['MSFT'], daily_df=daily_data['MSFT'], timespan='30T')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_diurnal_profile(diurnal_profile, q5, q25, q50, q75, q95):\n",
    "    \"\"\"Plot the diurnal profile with specified intervals on the x-axis.\"\"\"\n",
    "\n",
    "    times = sorted(diurnal_profile.index.unique())\n",
    "    \n",
    "    times_as_datetime = [pd.Timestamp.combine(pd.Timestamp.today(), time) for time in times]\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    plt.plot(times_as_datetime, q50, label=\"Median (50th percentile)\", color='blue')\n",
    "    \n",
    "    # Filling the area between the 5th and 95th percentiles and 25th and 75th percentiles\n",
    "    plt.fill_between(times_as_datetime, q5, q95, color='red', alpha=0.3, label=\"5th to 95th percentile\")\n",
    "    plt.fill_between(times_as_datetime, q25, q75, color='green', alpha=0.3, label=\"25th to 75th percentile\")\n",
    "    \n",
    "    # Set the x-axis labels as our interval times\n",
    "    plt.xticks(times_as_datetime, [time.strftime('%H:%M') for time in times_as_datetime], rotation=45)\n",
    "    \n",
    "    # Titles and labels\n",
    "    plt.title(\"Diurnal Profile of Garman-Klass Volatility\")\n",
    "    plt.xlabel(\"Time of Day\")\n",
    "    plt.ylabel(\"Normalized Volatility\")\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diurnal_profile, q5, q25, q50, q75, q95 = intraday_vol_aapl.calculate_diurnal_profile()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_diurnal_profile(diurnal_profile,q5, q25, q50, q75, q95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(aapl_daily['garch'], label='GARCH')\n",
    "plt.plot(aapl_daily['true_vol'], label='true')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aapl_minute = minute_data['AAPL']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aapl_aggregate = aapl_minute.groupby(aapl_minute.index.floor('5T')).aggregate({'open': 'first', \n",
    "                                                               'high': 'max', \n",
    "                                                               'low': 'min', \n",
    "                                                               'close': 'last'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aapl_aggregate['log_ret'] = np.log(aapl_aggregate['close']).diff()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aapl_daily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from arch import arch_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aapl_daily['log_ret_scaled'] = aapl_daily['log_ret'] * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(aapl_daily['true_vol'], label='True Vol')\n",
    "plt.plot(aapl_daily['ewma_vol'], label='ewma Forecasted Vol')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aapl_daily['true_vol'] = aapl_aggregate.groupby(aapl_aggregate.index.date).aggregate({'log_ret': 'sum'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timespan = '1H' \n",
    "aapl_vol_model = IntradayVol(hf_df=aapl_min, lf_df=aapl_hourly, daily_df=aapl_daily, timespan=timespan)\n",
    "msft_vol_model = IntradayVol(hf_df=msft_min, lf_df=msft_hourly, daily_df=msft_daily, timespan=timespan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AAPL Volatility Calculations\n",
    "aapl_daily_vol = aapl_vol_model.calculate_daily_vol()\n",
    "aapl_diurnal_profile = aapl_vol_model.calculate_diurnal_profile()\n",
    "aapl_predicted_vol = aapl_vol_model.predict_vol()\n",
    "\n",
    "# MSFT Volatility Calculations\n",
    "msft_daily_vol = msft_vol_model.calculate_daily_vol()\n",
    "msft_diurnal_profile = msft_vol_model.calculate_diurnal_profile()\n",
    "msft_predicted_vol = msft_vol_model.predict_vol()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using the Garma Klass vol formula as target\n",
    "aapl_predicted_vol['target'] = (np.sqrt(0.5 * (np.log(aapl_predicted_vol['high']) - np.log(aapl_predicted_vol['low']))**2 -\n",
    "                              (2 * np.log(2) - 1) * (np.log(aapl_predicted_vol['close']) - np.log(aapl_predicted_vol['open']))**2)).shift(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 10))\n",
    "plt.plot(aapl_predicted_vol['vol_forecasts'], label='Forecasted Volatility')\n",
    "plt.plot(aapl_predicted_vol['target'], label='Target Volatility')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 10))\n",
    "plt.plot(aapl_predicted_vol['vol_forecasts'], label='Forecasted Volatility')\n",
    "plt.plot(np.abs(aapl_predicted_vol['log_ret']), label='Abs Log Ret')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aapl_predicted_vol2 = aapl_predicted_vol.iloc[15:-1].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse(aapl_predicted_vol2['vol_forecasts'], aapl_predicted_vol2['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
