{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import yaml\n",
    "\n",
    "sys.path.append(os.getenv(\"CODE_PATH\"))\n",
    "sys.path.append(os.getenv(\"FIN_DATABASE_PATH\"))\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import logging\n",
    "from dotenv import load_dotenv\n",
    "from log_config import setup_logging\n",
    "from Data.connect import engine, DailyStockData, HourlyStockData, OneMinuteStockData, FiveMinuteStockData,FifteenMinuteStockData, StockSplits, StockNews, CompanyFinancials\n",
    "from data_fetcher import DataFetcher\n",
    "import yfinance as yf\n",
    "import matplotlib.pyplot as plt\n",
    "from pandas.tseries.offsets import DateOffset\n",
    "import statsmodels.api as sm\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fetching the data from SQL for AAPL and MSFT\n",
    "tickers = ['AAPL','MSFT', 'DIS', 'JPM', 'DIS', 'SPY']\n",
    "data = DataFetcher(tickers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vix = yf.download('^VIX',period = 'max', interval= '1d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minute_data = data.get_stock_data(timespan = 'hour')\n",
    "apple_min = minute_data['AAPL']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apple_min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# daily_data = data.get_stock_data() #default timespan is daily\n",
    "# apple_daily = daily_data['AAPL']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseProcessing:\n",
    "    def __init__(self, data: pd.DataFrame, iv_data: pd.DataFrame = None, period: str = '5T'):\n",
    "        self.original_data = data.copy() \n",
    "        self.data = data.copy()  \n",
    "        self.iv_data = iv_data \n",
    "        self.period = period\n",
    "    \n",
    "    def reset_data(self):\n",
    "        \"\"\"Reset data to the original state.\"\"\"\n",
    "        self.data = self.original_data.copy()\n",
    "        \n",
    "    def set_period(self, period: str = '5T'):\n",
    "        \"\"\"Set a new period for data aggregation.\"\"\"\n",
    "        self.period = period\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreProcessing(BaseProcessing):\n",
    "    def __init__(self, data: pd.DataFrame, iv_data: pd.DataFrame, period: str = '5T'):\n",
    "        super().__init__(data, iv_data,period)\n",
    "        \n",
    "        \n",
    "    def filter_market_hours(self):\n",
    "        \"\"\"Filtering the dataframe by market hours. Including 09:30 to 16:05 as\n",
    "        per Gallo and Browlees 2006 paper recommendaton \"\"\"\n",
    "        self.data['date'] = pd.to_datetime(self.data['date'])\n",
    "        self.data['day'] = self.data['date'].dt.date\n",
    "        self.data['time'] = self.data['date'].dt.time\n",
    "        start_time = datetime.strptime('09:30', '%H:%M').time()\n",
    "        end_time = datetime.strptime('16:05', '%H:%M').time()\n",
    "        self.data = self.data[\n",
    "            (self.data['time'] >= start_time) & (self.data['time'] <= end_time)].copy()\n",
    "        self.data.drop(columns=['time'], inplace = True)\n",
    "        return self\n",
    "    \n",
    "    \n",
    "    def setting_index(self):\n",
    "        \"\"\"Set date column as index for the dataframe\"\"\"\n",
    "        self.data.set_index('date', inplace = True)\n",
    "        return self\n",
    "    \n",
    "    def aggregate_min_data(self):\n",
    "        \"\"\" Aggregating the data into intervals.\"\"\"\n",
    "        self.data = self.data.groupby(self.data.index.floor(self.period)).aggregate({\n",
    "            'open': 'first',\n",
    "            'high': 'max',\n",
    "            'low' : 'min',\n",
    "            'close': 'last'\n",
    "        })\n",
    "        return self\n",
    "    \n",
    "    def preprocess_iv(self):\n",
    "        self.iv_data.index = pd.to_datetime(self.iv_data.index)\n",
    "        ticker_dates = self.data.index.date.unique()\n",
    "        self.iv_data = self.iv_data[self.iv_data.index.isin(ticker_dates)]\n",
    "        return self.iv_data\n",
    "    \n",
    "\n",
    "    def threshold_filter(self, threshold_ratio = 0.85):\n",
    "        \"\"\"\n",
    "        Filter the data for days with at least 85% of data\n",
    "        (85% used as suggested in Hensen and Huang's paper).\n",
    "        \n",
    "        Args:\n",
    "            threshold (int): Minimum number of  intervals required for a day to be considered valid.\n",
    "        \"\"\"\n",
    "        trading_day = 395 # 9:30 to 16:05\n",
    "        intervals_per_day = trading_day // (pd.Timedelta(self.period).seconds // 60) #intervals per day\n",
    "        \n",
    "        threshold = int(intervals_per_day * threshold_ratio)\n",
    "        \n",
    "        daily_counts = self.data.groupby(self.data.index.normalize()).size()\n",
    "        valid_days = daily_counts[daily_counts >= threshold].index\n",
    "        self.data = self.data[self.data.index.normalize().isin(valid_days)]\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    \n",
    "    def pre_processor(self):\n",
    "        \"\"\"Executes the full preprocessing pipeline.\n",
    "        \"\"\"\n",
    "        return self.filter_market_hours().setting_index().aggregate_min_data().threshold_filter().data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelInputs(BaseProcessing):\n",
    "    \"\"\" Class to compute the necessary inputs for the HAR model\n",
    "        data: Pre Processed data \"\"\"\n",
    "    def __init__(self, data: pd.DataFrame, period: str = '5T'):\n",
    "        super().__init__(data, period)\n",
    "        \n",
    "    \n",
    "    def calculate_return(self):\n",
    "        self.data['return'] = self.data['close'].pct_change()\n",
    "        self.data.dropna(subset =['return'], inplace = True)\n",
    "        self.data['squared_return'] = self.data['return']**2\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def set_target(self):\n",
    "        self.data['target'] = 0.0  # Initialize as float\n",
    "        \n",
    "        non_zero_mask = self.data['squared_return'] != 0\n",
    "        self.data.loc[non_zero_mask, 'target'] = np.log(self.data.loc[non_zero_mask, 'squared_return'])\n",
    "        \n",
    "        self.data['target'] = self.data['target'].shift(-1)\n",
    "        self.data.dropna(subset=['target'], inplace=True)\n",
    "        \n",
    "        return self\n",
    "\n",
    "    \n",
    "    def realised_vol(self):\n",
    "        \"\"\"Calculating daily Realised Volatility and HAR components.\"\"\"\n",
    "        \n",
    "        trading_day = 395 # 9:30 to 16:05\n",
    "        intervals_per_day = trading_day // (pd.Timedelta(self.period).seconds // 60) #intervals per day\n",
    "        self.data['LRV_lag1'] = self.data['squared_return'].shift(1)\n",
    "        self.data['LRV_lag1'] = np.where(self.data['squared_return'].shift(1) == 0, 0, self.data['LRV_lag1'])\n",
    "\n",
    "        self.data['LRV_daily'] = np.log(self.data['squared_return'].rolling(window = intervals_per_day).sum())\n",
    "        self.data['LRV_weekly'] = np.log(self.data['squared_return'].shift(1).rolling(window = intervals_per_day * 4).mean())\n",
    "        self.data['LRV_monthly'] = np.log(self.data['squared_return'].shift(5).rolling(window = intervals_per_day * 17).mean())\n",
    "        \n",
    "        return self\n",
    "\n",
    "        \n",
    "    \n",
    "    def diurnal_profile(self, window = 21):\n",
    "        \"\"\"Calculate diurnal profile based on the Garman-Klass vol for \n",
    "        each time bucket.\n",
    "        Using quantiles as it is more robust to outliers.\"\"\"\n",
    "        self.data['garma_klass_vol'] = np.sqrt(0.5*(np.log(self.data['high']) - np.log(self.data['low']))**2 - (2 * np.log(2) -1) * \n",
    "        (np.log(self.data['close']) - np.log(self.data['open']))**2)\n",
    "        \n",
    "        grouped = self.data.groupby(self.data.index.time)['garma_klass_vol']\n",
    "        \n",
    "        q25 = grouped.apply(lambda x: x.rolling(window=window).quantile(0.25))\n",
    "        q50 = grouped.apply(lambda x: x.rolling(window=window).quantile(0.5))\n",
    "        q75 = grouped.apply(lambda x: x.rolling(window=window).quantile(0.75))\n",
    "        \n",
    "        diurnal_profile = (q25 + q50 + q75) / 3\n",
    "        diurnal_profile_standardized = diurnal_profile / diurnal_profile.mean() # standardising it\n",
    "        \n",
    "        # This returns a multiindex dataframe, we only need the second index to merge\n",
    "        diurnal_profile_df = pd.DataFrame({'diurnal_profile': diurnal_profile_standardized}).reset_index(level = 0, drop = True) \n",
    "        \n",
    "        self.data['time_of_day'] = self.data.index.time\n",
    "        \n",
    "        self.data = self.data.merge(diurnal_profile_df, how='left', right_index=True, left_index = True)\n",
    "        self.data.dropna(subset = ['diurnal_profile'], inplace = True) # Dropping nan from the dataset. We will lose roughly a month of observations \n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def preprocess(self):\n",
    "        return self.calculate_return().set_target().realised_vol().diurnal_profile().data\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess = PreProcessing(apple_min,vix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apple_prepocessed = preprocess.pre_processor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = ModelInputs(apple_prepocessed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apple_data = inputs.preprocess()\n",
    "apple_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apple_data[apple_data['time_of_day'] == '09:35:00']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(apple_data['diurnal_profile'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['LRV_lag1', 'LRV_daily', 'LRV_weekly', 'LRV_monthly', 'diurnal_profile']\n",
    "target = 'target'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start the training at the beginning of our dataset. We then train for one year test on the other one.\n",
    "Then we add one year to the training set and test on the following one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VolPrediction:\n",
    "    def __init__(self, data: pd.DataFrame, train_size:int = 1, val_size:int = None, test_size:int = 1):\n",
    "        self.data = data.copy()\n",
    "        self.train_size = train_size\n",
    "        self.val_size = val_size\n",
    "        self.test_size = test_size\n",
    "        \n",
    "        self.predictions = []\n",
    "        self.true_values = []\n",
    "\n",
    "    def rolling_predict(self, features, target):\n",
    "        \n",
    "        self.data['prediction'] = np.nan\n",
    "        \n",
    "        current_train_start = self.data.index.min()\n",
    "        end_date = self.data.index.max()\n",
    "        \n",
    "        while current_train_start + DateOffset(years = self.train_size + self.test_size) <= end_date:\n",
    "        \n",
    "            current_train_end = current_train_start + DateOffset(years = self.train_size) - DateOffset(days = 1)\n",
    "            current_test_end = current_train_end + DateOffset(years = self.test_size)\n",
    "            \n",
    "            train_data = self.data.loc[current_train_start:current_train_end]\n",
    "            test_data = self.data.loc[current_train_end + DateOffset(days = 1):current_test_end]\n",
    "            \n",
    "            means = train_data[features].mean()\n",
    "            stds = train_data[features].std()\n",
    "            \n",
    "            X_train = (train_data[features] - means) / stds\n",
    "            y_train = train_data[target]    \n",
    "            \n",
    "            X_test = (test_data[features] - means) / stds\n",
    "            y_test = test_data[target]\n",
    "\n",
    "            model = LinearRegression()\n",
    "            model.fit(X_train, y_train)\n",
    "            \n",
    "            y_pred = model.predict(X_test)  \n",
    "            \n",
    "            self.predictions.extend(y_pred)\n",
    "            self.true_values.extend(y_test.values)\n",
    "            \n",
    "            self.data.loc[test_data.index,'prediction']  = y_pred \n",
    "            \n",
    "            print(f'Training on data from {current_train_start} to {current_train_end}')\n",
    "            print(f'Testing on data from {current_train_end + DateOffset(days=1)} to {current_test_end}')\n",
    "            \n",
    "            current_train_start = current_train_start + DateOffset(years=self.test_size)\n",
    "            \n",
    "        return self.data, self.predictions, self.true_values\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicting = HAR(apple_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extended HAR - Prediction\n",
    "RV(i,t+h) = alpha + B * Diurnal component \n",
    "\n",
    "here, t is a time, like 10:30, 10:35 etc... h is the window, so t+h is \n",
    "the next prediction bucket, so if we it is 11 am and h is 5 min, the prediction\n",
    "is 11:05, and the Diurnal component of 11:05 is one of the features to incorporate\n",
    "into the model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
